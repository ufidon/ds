# Complexity analysis
- Time and space complexity
  - Best, average and worst case
- Asymptotic analysis
  - Big ${Î˜, O, Î©}$ notations


Algorithm executing Time 
---
- can be used to measure the performance of algorithms
  - by profilers
- two problems
  - depends on execution environment
  - depends on specific input




Growth rate of cost vs. input size
---
- a theoretical measure of algorithm performance
- `the time or storage consumed` by a algorithm increases when `the size of input or problem scale` grows
  - $p=f(inputSize)$
  - the *order of magnitude* of function $f$ can be denoted by Big ${Î˜, O}$ or ${Î©}$
  - it uses `logical units` instead of real time units or storage units
- provides a good theoretical estimate of algorithm efficiency
  - independent of specific hardware, programming language or conditions
  - but two algorithms of the same time complexity are not necessarily equally efficient


ğŸ’¡Demo: dominant term
---
- Given $f(n)=9n^2 + 99n + 999\log n + 9999$, which component is dominant when $n$ grows large?

![dominant term](./images/domc.png)

- when $nâ†’âˆ, f(n)$ is dominated by $9n^2$, so its `asymptotic behavior` can be described by
  - `dropping the non-dominant terms`: $f(n)=9n^2 + 99n + 999\log n + 9999 âˆˆ Î˜(9n^2)$
  - it can be simplified further by `dropping the constant coefficient`: $Î˜(9n^2) âˆˆ Î˜(n^2)$
  - âˆ´ $f(n)âˆˆÎ˜(n^2)$
  - âš ï¸ `âˆˆ` is used here because $Î˜(n^2)$ denotes `the set of all functions` asymptotically proportional to $n^2$ 
- $f(n)âˆˆÎ˜(n^2)$ describes that $f(n)$ is `proportional` to $n^2$ when n is large enough
  - i.e $\displaystyle \lim_{nâ†’âˆ}\dfrac{f(n)}{n^2}=c$ 
    - $c$ is a `non-zero` constant
- A constant function $f(n)=c$ is always proportional to 1 so $f(n)âˆˆÎ˜(c)âˆˆÎ˜(1)$


Asymptotic analysis
---
- describes the behavior of a function near its limits
  - how a function behaves as its input value becomes very large
- e.g.
  - Stirlingâ€™s Formula: $\displaystyle n! \approx \sqrt{2 \pi n} \left(\frac{n}{e}\right)^n$
    - $\displaystyle n!âˆˆÎ˜(\sqrt{n} \left(\frac{n}{e}\right)^n)$
  - Harmonic Numbers: $\displaystyle H_n = \sum_{k=1}^n \frac{1}{k} â‰ˆ \ln n + Î³$
    - Î³ is Eulerâ€™s constant
    - $H_nâˆˆÎ˜(\ln n)$


Big ${Î˜, O, Î©}$
---
![asyms](./images/asyms.png)

| Notation  | Short Notation | Representation   | Definition  |
| ---------- | ------------ |  -------- | -------- |
| Big O:<br> $f(n)âˆˆO(g(n))$ | $f(n)âªg(n)$   | an `asymptotical upper bound` on the `growth rate` of the function $f(n)$ as $nâ†’âˆ$. |  $f(n)$ is $O(g(n))$ if there exist constants $c$ and $n_0$ such that $f(n) â‰¤ c g(n)$ for all $n â‰¥ n_0$. |
| Big Omega:<br> $f(n)âˆˆÎ©(g(n))$ | $f(n)âªg(n)$   | an `asymptotical lower bound` on the `growth rate` of the function $f(n)$ as $nâ†’âˆ$. | $f(n)$ is $Î©(g(n))$ if there exist constants $c$ and $n_0$ such that $f(n) â‰¥ c g(n)$ for all $n â‰¥ n_0$. |
| Big Theta:<br> $f(n)âˆˆÎ˜(g(n))$ | $f(n)âˆ¼g(n)$   |  an `asymptotical proportional bound` on the `growth rate` of the function $f(n)$ as $nâ†’âˆ$. | $f(n)$ is $Î˜(g(n))$ if there exist constants $c_1, c_2$, and $n_0$ such that $c_1  g(n) â‰¤ f(n) â‰¤ c_2  g(n)$ for all $n â‰¥ n0$. |


â˜¯ï¸ Theorem 
---
Let's  
- denote $f(n)âˆˆÎ˜(g(n))$ by $f(n)âˆ¼g(n)$, $f(n)$ is `asymptotically proportional` to $g(n)$
- denote $f(n)âˆˆO(g(n))$ by $f(n)âªg(n)$, $g(n)$ is an `asymptotical upper bound` of $f(n)$ 
- denote $f(n)âˆˆÎ©(g(n))$ by $f(n)âªg(n)$, $g(n)$ is an `asymptotical lower bound` of $f(n)$ 

then
- $f(n)âˆ¼g(n)$ if and only if $f(n)âªg(n)$ and $f(n)âªg(n)$
  - i.e. $g(n)âªf(n)âªg(n)$, $g(n)âªf(n)âªg(n)$, $f(n)âªg(n)âªf(n)$, and $f(n)âªg(n)âªf(n)$
- $f(n)âªg(n)$ if and only if $g(n)âªf(n)$
- if $f_1âˆ¼f_2$ and $f_2âˆ¼f_3$ then $f_1âˆ¼f_3$
- if $f_1âªf_2$ and $f_2âªf_3$ then $f_1âªf_3$
- if $f_1âªf_2$ and $f_2âªf_3$ then $f_1âªf_3$


Best, Worst, and Average Cases 
---
- `best-case`: results in the `shortest` execution time
- `worst-case`: results in the `longest` execution time 
  - determines the `lower bound` of performance
  - easier to obtain and commonly used
- `average-case`: 
  - attempts to determine the `average` amount of time among `all possible inputs` of the same size
  - ideal, but difficult to perform
    - hard to determine the relative `probabilities and distributions` of various input instances for many problems
- each case can described by any one of ${Î˜, O}$, or $Î©$
  - $Î˜$ is preferred, unfortunately, $O$ is widely used

<table>
  <thead>
    <tr>
      <th>Data Structure</th>
      <th>Operation</th>
      <th>Worst Case</th>
      <th>Average Case</th>
      <th>Best Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="3">Array</td>
      <td>Insertion (at end)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
    </tr>
    <tr>
      <td>Deletion (at end)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
    </tr>
    <tr>
      <td>Search</td>
      <td>Î˜(n)</td>
      <td>Î˜(n)</td>
      <td>Î˜(1)</td>
    </tr>
    <tr>
      <td rowspan="3">Linked List</td>
      <td>Insertion (at beginning)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
    </tr>
    <tr>
      <td>Deletion (at beginning)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
    </tr>
    <tr>
      <td>Search</td>
      <td>Î˜(n)</td>
      <td>Î˜(n)</td>
      <td>Î˜(1)</td>
    </tr>
    <tr>
      <td rowspan="3">Stack</td>
      <td>Push</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
    </tr>
    <tr>
      <td>Pop</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
    </tr>
    <tr>
      <td>Peek</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
    </tr>
    <tr>
      <td rowspan="3">Queue</td>
      <td>Enqueue</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
    </tr>
    <tr>
      <td>Dequeue</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
    </tr>
    <tr>
      <td>Peek</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
      <td>Î˜(1)</td>
    </tr>
  </tbody>
</table>



Example ğŸ’¡: find the growth rate of linear search
---
```c++
int linearSearch(int arr[], int n, int x)
{
  for (int i = 0; i < n; i++)
  {
    if (arr[i] == x)
    {
      return i; // Element found at index i
    }
  }
  return -1; // Element not found
}
```

- The execution time of Linear search is proportional to the size of the array
  - so it has an order of $n$
  - denoted as $Î˜(n)$
- analysis
  - ignore multiplicative constants
  - ignore non-dominating terms


Practice ğŸ“ : determine Big-Î˜
---
- Repetition
- Sequence 
- Selection
- Logarithm


Repetition: simple loops
---
```c++
// case 1
for(i=1; i<=n; i++){
  s += i;
}

// case 2
for(i=n; i>=-10; i--){
  s |= i;
}

// case 3
i= 99*n;
while(i--);

// case 4
for(i=99; i<=9*n; i += 2){
  s += i;
}
```
- Time complexity: $T(n) = cn = Î˜(n)$

```c++
// case 1
for(i=1; i<=n; i++){
  s += i;
  n /= 2;
}

// case 2
while(n>9999){
  n /= 2;
}

// case 3
while(k<12345*n){
  k *= 2;
}
```
- Time complexity: $T(n) = c\log_2 n = Î˜(\log_2n)$

Repetition: nested loops
---
```c++
// case 1
for(i=1; i<=n; i++){
  for(j=1; j<=n; j++){
    for(k=1; k<=n; k++){
      c[i][j] += a[i][k]*b[k][j];
    }
  }

// case 2
for(i=9; i<n; i++)
  for(j=99; j<n*n; j++)
    cout <<"Who am I?"<< endl;

// case 3
for(i=9; i<3*n*n; i++)
  for(j=99; j<7*n; j++)
    cout <<"Who am I?"<< endl;
```
- Time complexity: $T(n) = cn^3 = Î˜(n^3)$

```c++
// case 1
for(i=1; i<=n; i *= 3)
  for(j=1; j<=n; j++)
    cout <<"Who am I?"<< endl;

// case 2
for(i=9; i<n*3; i++)
  for(j=99; j<n*4; j *= 3)
    cout <<"Who am I?"<< endl;
```
- Time complexity: $T(n) = cn\log_3 n = Î˜(n\log_3 n)$

- matrix multiplication: $C=A\times B$
  ```c++
  for(i=1; i<=n; i++){
    for(j=1; j<=n; j++){
      for(k=1; k<=n; k++){
        c[i][j] += a[i][k]*b[k][j];
      }
    }
  ```
- Time complexity: $T(n) = cn^3 = Î˜(n^3)$

- partial sum: ${\displaystyle S[i] = \sum_{k=1}^{i} k}$
  ```c++
  for(i=1; i<=n; i++){
    for(k=1; k<=i; k++){
      S[i] += k;
    }
  }
  ```
- Time complexity: ${\displaystyle T(n) = c\frac{n(n+1)}{2} = Î˜(n^2)}$  

- print a table: n rows x K columns
  ```c++
  for(i=1; i<=n; i++){
    for(k=1; k<=K; k++){
      cout << T[i][k];
    }
    cout<<endl;
  }
  ```
- Time complexity: $T(n) = cKn = c'n = Î˜(n)$ 


Sequence
---
- print a table with a header: n rows x K columns
  ```c++
  for(k=1; k<= K; k++){
    cout << H[k];
  }

  for(i=1; i<=n; i++){
    for(k=1; k<=K; k++){
      cout << T[i][k];
    }
    cout<<endl;
  }
  ```
- Time complexity: $T(n) = cK + cKn \approx  c'n = Î˜(n)$ 


Searching
---
- ğŸ’¡ Intuition by animation
  - [linear search in arbitrary array](https://liveexample.pearsoncmg.com/dsanimation13ejava/LinearSearcheBook.html)
  - [linear search in sorted array](https://www.cs.usfca.edu/~galles/visualization/Search.html)
- `Linear search` for e in list
  ```c++
  bool found = false;
  for(auto& o: list){
    if (o == e){
      found = true;
      cout<<e<<endl;
    }     
  }

  if(!found) {
    for(auto& o: list){
      cout << o;
    }
  }
  ```
- Time complexity: $T(n)=Î˜(n)+Î˜(n)=Î˜(n)$
  - An algorithm with the  $Î˜( n)$ time complexity is called a *linear algorithm*

- Logarithm time: `Binary search` in an ordered array
- ğŸ’¡ Intuition by animation
  - [binary search in short array](https://liveexample.pearsoncmg.com/dsanimation13ejava/BinarySearcheBook.html)
  - [binary search](https://www.cs.usfca.edu/~galles/visualization/Search.html)
  ```c++
  int bsearch(int a[], int t, int left, int right){
    while(left < right){
      int m = (left+right)/2;
      if(a[m] == t)
        return m;
      else if(a[m] < t)
        return bsearch(a, t, m+1, right);
      else
        return bsearch(a, t, left, m-1);
    }
  }
  ```
- Time complexity: $T(n)=T(\frac{n}{2})+c = Î˜(\log n)$ 
  - An algorithm with the  $Î˜(\log n)$ time complexity is called a *logarithmic algorithm*

Selection sort
---
- ğŸ’¡ Intuition by animation
  - [with pseudocode](https://csvistool.com/SelectionSort)
  - [without pseudocode](https://liveexample.pearsoncmg.com/dsanimation13ejava/SelectionSorteBook.html)
- Find the minimum from the unsorted part and append it to the sorted part
  ```c++
  for(i=1; i<=n-1; i++){
    int min = a[i+1];
    int minIndex = i+1;
    for(j=i+1; j<=n; j++){
      if(a[j] < min){
        minIndex = j; 
        min = a[j];        
      }
    }
    swap(a[i], a[minIndex]);
  }
  ```
- Time complexity: ${\displaystyle T(n) = (n-1) + c + (n-2) + c + \cdots + 1+c=\frac{(n-1)n}{2}+(n-1)c = Î˜(n^2)}$
  - An algorithm with the $Î˜(n^2)$ time complexity is called a *quadratic algorithm*


Tower of Hanoi
---
- ğŸ’¡ Intuition by animation
  - [normal case](https://yongdanielliang.github.io/animation/web/TowerOfHanoi.html)
  - [extended case](https://towersofhanoi.info/Animate.aspx)
- moves n disks from tower A to tower B via tower C recursively as follows:
  - Move the first n â€“ 1 disks from A to C via B
  - Move disk n from A to B
  - Move n - 1 disks from C to B via A
- no larger disk is put on smaller one at each move

```c++
void Hanoi(n, A, B, C){
  Hanoi(n-1, A, C, B);
  cout << "Disk " << n << ":" << A <<"->" << B;
  Hanoi(n-1, C,B,A);
}
```
- Time complexity: $T(n) = T(n-1)+c+T(n-1) \Rightarrow T(n) = Î˜(2^n)$


Common Recurrence Relations
---
| Recurrence relation | Result | Example |
| --- | --- | --- |
| $T(n)=T(\frac{n}{2})+Î˜(1)$ | $T(n)=Î˜(\log n)$ | Binary search, Euclid's GCD |
| $T(n)=T(n-1)+Î˜(1)$ | $T(n)=Î˜(n)$ | Linear search |
| $T(n)=2T(\frac{n}{2})+Î˜(1)$ | $T(n)=Î˜(n)$ |  |
| $T(n)=2T(\frac{n}{2})+Î˜(n)$ | $T(n)=Î˜(n\log n)$ | Merge sort |
| $T(n)=2T(\frac{n}{2})+Î˜(n\log n)$ | $T(n)=Î˜(n\log^2 n)$ |  |
| $T(n)=T(n-1)+Î˜(n)$ | $T(n)=Î˜(n^2)$ | Selection sort, insertion sort|
| $T(n)=2T(n-1)+Î˜(1)$ | $T(n)=Î˜(n^2)$ | Tower of Hanoi |
| $T(n)=T(n-1) + T(n-2) +Î˜(1)$ | $T(n)=Î˜(n^2)$ | Recursive Fibonacci algorithm |

Order of growth of common functions
---
- $f(n) â‰º g(n)$ and $g(n)â‰»f(n)$ means $g(n)$ `dominates` $f(n)$ so $c_1f(n)$ in $c_1f(n) + c_2g(n)$ can be omitted in asymptotical analysis
  - $c_1,c_2$ are non-zero constants
- $\displaystyle f(n)â‰ºg(n)$ and $\displaystyle g(n)â‰»f(n)$  represents $\displaystyle\lim_{nâ†’âˆ}\dfrac{f(n)}{g(n)}=0$


$$1 â‰º \log n â‰º \sqrt{n} â‰º n â‰º n\log n â‰º n^2 â‰º n^3 â‰º 2^n â‰º 3^n â‰º n!$$

$$n! \succ 3^n \succ 2^n \succ n^3 \succ n^2 \succ n\log n \succ n \succ \sqrt{n} \succ \log n \succ 1
$$

![common functions](./images/stds.png)

Relationship between order of growth and ${Î˜, O, Î©}$
---
- $f(n)â‰ºg(n) â†’ f(n)âªg(n)$ and $g(n)âªf(n)$ but NOT $f(n)âˆ¼g(n)$
  - the converse is NOT true, i.e. $f(n)âªg(n)$ or $g(n)âªf(n) â†› f(n)â‰ºg(n)$
- $1 âª \log n âª \sqrt{n} âª n âª n\log n âª n^2 âª n^3 âª 2^n âª 3^n âª n!$
- $n! âª 3^n âª 2^n âª n^3 âª n^2 âª n\log n âª n âª \sqrt{n} âª \log n âª 1$


ğŸƒ Practice
---
List the following functions by `complexity categroy` in the `increasing order of efficiency`:
- $7n\log (3n+99), 3n^{5/2}, 4n^2+9n+99, 99â‹…2^n, 4^n/9999, 3â‹…n^{10}+4â‹…10^n, 2024â‹…n!+999^n$

Solution:
- $2024â‹…n!+999^nâˆ¼n!+999^nâˆ¼n!$
- $3â‹…n^{10}+4â‹…10^nâˆ¼n^{10}+10^nâˆ¼10^n$
- $4^n/9999âˆ¼4^n$
- $99â‹…2^nâˆ¼2^n$
- $3n^{5/2}âˆ¼n^{5/2}âˆ¼n^{2.5}$
- $4n^2+9n+99âˆ¼n^2+nâˆ¼n^2$
- $7n\log(3n+99)âˆ¼n\log 3nâˆ¼n\log 3+n\log nâˆ¼n+n\log nâˆ¼n\log n$


ğŸƒ Practice
---
Prove $âˆ€a,b>1: \log_a n âˆ¼ \log_b n$.

Proof:
- $\displaystyle \log_a n = \frac{\ln n}{\ln a} âˆ¼ \ln n$
- $\displaystyle \log_b n = \frac{\ln n}{\ln b} âˆ¼ \ln n$
- âˆ´ $\log_a n âˆ¼ \log_b n$


Recursive [Fibonacci](https://en.wikipedia.org/wiki/Fibonacci_sequence) algorithm
---
- ğŸ’¡ [animation](https://web.eecs.utk.edu/~czheng4/viz/animations/recursion/fibonacci/fibonacci.html)
- $F_0=0, F_1=1, F_n=F_{n-1}+F_{n-2}$
```c++
long fib(long n){
  if(n==0) return 0;
  if(n==1) return 1;
  return fib(n-1)+fib(n-2);
}
```
- Time complexity: $T(n) = T(n-1)+T(n-2) +c \Rightarrow T(n) = Î˜(2^n)$


Fibonacci sequence by *dynamic programming*
---
- recursive problems can be solved in two popular ways
  - top-down: divide and conquer (recursive algorithm)
    - inefficient due to repeatedly computing subproblems
  - bottom-up: dynamic programming
    - solve each subprogram only once 
    - store the results for later use to avoid repeated computing
    - solve subproblems first, then combine the solutions to obtain the overall solution

```c++
long fib(long n){
  long f0=0, f1=1, f2=1;
  if(n==0) return f0;
  else if(n==1) return f1;
  else if (n==2) return f2;

  for(int i=3; i<= n; i++){
    f0=f1;
    f1=f2;
    f2= f0+f1;
  }
  return f2;
}
```
  - Time complexity: $Î˜(n)$


Practice ğŸ“: find the *greatest common divisor (gcd)* of two integers
---
- Analyze the following algorithms and find their time complexities
- suppose $m\ge n$
- GCD algorithm version 1: brute force
  - check all possible divisors
  ```c++
  int gcd(int m, int n){
    int gcd = 1;
    for(int d=2; d<=m && d<=n; d++){
      if(m%k==0 && n%k==0) gcd = k;
    }
    return gcd;
  }
  ```
  - Time complexity: $Î˜(n)$
- GCD algorithm version 2: brute force
  - check half of all possible divisors
  ```c++
  int gcd(int m, int n){
    int gcd = 1;
    for(int d=min(m,n); d>=1; d--){
      if(m%k==0 && n%k==0){
        gcd = k;
        break;
      } 
    }
    return gcd;
  }
  ```
  - Time complexity: $Î˜(n)$


[Euclid's algorithm](https://en.wikipedia.org/wiki/Euclidean_algorithm)
---
- Let gcd(m, n) denote the gcd for integers m and n:
  - If m % n is 0, gcd (m, n) is n
  - Otherwise, gcd(m, n) is gcd(n, m % n)

  ```c++
  int gcd(int m, int n){
    if(m%n == 0)
      return n;
    else
      return gcd(n, m%n);
  }
  ```
  - Time complexity: $Î˜(\log n)$


Find all prime numbers no larger than n
---
- divide each integer i from 2 to ${\displaystyle \frac{i}{2}}$
  - check all possible divisors
  ```c++
  for(int i=2; i<= n; ++i){
    boolean isprime = true;
    for(int d=2; d<= i/2; ++d){
      if(i%d == 0){
        isprime = false;
        break;
      }
    }
    if(isprime) cout<<i<<endl;
  }
  ```
  - Time complexity: $Î˜( n^2)$
- divide each integer i  from 2 to $\sqrt{i}$
  - check divisors up to $\sqrt{n}$
  ```c++
  for(int i=2; i<= n; ++i){
    boolean isprime = true;
    for(int d=2; d<= Math.sqrt(i); ++d){
      if(i%d == 0){
        isprime = false;
        break;
      }
    }
    if(isprime) cout<<i<<endl;
  }
  ```
  - Time complexity: $Î˜(n\sqrt{n})$
- Improved: divide each prime integer i  from 2 to $\sqrt{i}$
  - check prime divisors up to $\sqrt{n}$
  ```c++
  #include <iostream>
  #include <vector>

  int main()
  {
    int n;
    std::cout << "Enter the value of n: ";
    std::cin >> n;

    std::vector<int> primes;
    for (int i = 2, squareRoot = 1; i <= n; ++i)
    {
      bool isPrime = true;
      if (squareRoot * squareRoot < i)
        ++squareRoot;

      for (size_t k = 0; k < primes.size() && primes[k] <= squareRoot; k++)
      {
        if (i % primes[k] == 0)
        {
          isPrime = false;
          break;
        }
      }

      if (isPrime)
      {
        primes.push_back(i);
        std::cout << i << std::endl;
      }
    }

    return 0;
  }
  ```
  - Time complexity: ${\displaystyle Î˜(\frac{n\sqrt{n}}{\log n})}$
    - derived by ${\displaystyle \pi(n) \approx \frac{n}{\log n}}$
    - where $\pi(n)$ denotes the number of prime numbers no larger than $n$ 
- [Sieve of Eratosthenes](https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes)
  ```c++
  #include <iostream>
  #include <vector>

  int main()
  {
    int n;
    std::cout << "Enter the value of n: ";
    std::cin >> n;

    std::vector<bool> primes(n + 1, true);

    for (int k = 2; k * k <= n; ++k)
    {
      if (primes[k])
      {
        for (int i = k; k * i <= n; i++)
        {
          primes[k * i] = false;
        }
      }
    }

    for (int k = 2; k <= n; ++k)
    {
      if (primes[k])
      {
        std::cout << k << std::endl;
      }
    }

    return 0;
  }
  ```
  - Time complexity: ${\displaystyle Î˜(\frac{n\sqrt{n}}{\log n})}$



Practice ğŸ“: solve the [Closest-pair problem](https://en.wikipedia.org/wiki/Closest_pair_of_points_problem) with divide-and-conquer
---
- Given a set of points, find the two points that are nearest to each other
  - ğŸ’¡ [intuition by animation](https://liveexample.pearsoncmg.com/dsanimation13ejava/ClosestPaireBook.html)
- brute force: computes the distances between all pairs of points and finds the one with the minimum distance
  - Time complexity: ${\displaystyle T(n)={n \choose 2} = \frac{n(n-1)}{2} = Î˜(n^2)}$
- divide-and-conquer
  - Step 1: sort the points in increasing order of x-coordinates then y-coordinates into a sorted list $S$ of points
    - Time complexity with mergesort: ${\displaystyle T(n) = Î˜(n\log n)}$
  - Step 2: Divide $S$ into two subsets $S_1$ and $S_2$ with its midpoint $m$
    - let $m$ in $S_1$, recursively find the closest pair $p_1$ and $p_2$ recursively
  - Step 3: Find the closest pair $p_3$ between $S_1$ and $S_2$. Then the closest pair is the closest one from $(p_1,p_2,p_3)$
    - Time complexity of Step 2 and Step 3: 
      - $`{\displaystyle T(n)=\underbrace{2T(\frac{n}{2})}_{Step\ 2} + \underbrace{Î˜(n)}_{Step 3} = Î˜(n\log n)}`$


Practice ğŸ“: solve the [eight queens problem](https://en.wikipedia.org/wiki/Eight_queens_puzzle) with backtracking
---
-  place a queen in each row on a chessboard such that no two queens can attack each other
   -  ğŸ’¡ [intuition by animation](https://liveexample.pearsoncmg.com/dsanimation13ejava/EightQueenseBook.html)
   -  many solutions
- algorithm 1: recursion
  - [source code](./demos/EightQueensRecursion.cpp)
  - extend it to find all solutions
- algorithm 2: backtracking
  - [source code](./demos/EightQueensBackTracking.cpp)
  - extend it to find all solutions
- generalize 8-queens problem to n-queens problem


Practice ğŸ“: Computational Geometry: the [convex hull problem](https://en.wikipedia.org/wiki/Convex_hull) with backtracking
---
- Given a set of $n$ points, a convex hull of size $h$ is the smallest convex polygon that encloses all these points
  - every line connecting two vertices is inside the polygon
  - ğŸ’¡ [intuition by animation](https://liveexample.pearsoncmg.com/dsanimation13ejava/ConvexHulleBook.html)
- [algorithms to be discussed](https://en.wikipedia.org/wiki/Convex_hull_algorithms)
  - Gift wrapping algorithm
    - [source code](./demos/ConvexHullbyGiftWrapping.cpp)
    - Time complexity: ${\displaystyle Î˜(nh)}$
  - Graham's algorithm
    - [source code](./demos/ConvexHullbyGraham.cpp)
    - Time complexity: ${\displaystyle Î˜(n\log n)}$


Practice ğŸ“: the [string search problem](https://en.wikipedia.org/wiki/String-searching_algorithm) 
---
- Find a substring or pattern of length $m$ in a given string or text of length $n$
  - two methods in the String class
    - contains(pattern) tests if a pattern is in the string
    - indexOf(pattern) returns the index of the first matching of the pattern in the string
- algorithms to be discussed
  - brute force
    - ğŸ’¡ [intuition by animation](https://csvistool.com/BruteForce)
    - [source code](./demos/StringMatch.cpp)
    - Time complexity: $Î˜(nm)$
  - Boyer-Moore algorithm
    - ğŸ’¡ [intuition by animation](https://csvistool.com/BoyerMoore)
    - [source code](./demos/StringMatchBoyerMoore.cpp)
    - Time complexity: $Î˜(nm)$
  - Knuth-Morris-Pratt  algorithm
    - ğŸ’¡ [intuition by animation](https://csvistool.com/KMP)
    - [source code](./demos/StringMatchKMP.cpp)
    - Time complexity: $Î˜(n + m)$


Objectives
---
- Estimate algorithm efficiency using the Big Î˜ notation
  -  explain growth rates and 
     -  why constants and nondominating terms can be ignored
  - describe common growth functions 
    - constant, logarithmic, log-linear, quadratic, cubic, and exponential
- Analyze and determine the complexity of various types of algorithms
  -  binary search
  -  selection sort
  -  Tower of Hanoi
- Design efficient algorithms for 
  - finding 
    - Fibonacci numbers using dynamic programming
    - the GCD using Euclidâ€™s algorithm
    - prime numbers using the sieve of Eratosthene
    - the closest pair of point using the divide-and-conquer approach
    - a convex hull for a set of points
  - solving
    -  the Eight Queens problem using the backtracking approach
    -  string matching using Boyer-Moor and KMP algorithms


# Math appendix
- ${\displaystyle \sum_{k=1}^n k = \frac{n(n+1)}{2}}$
- ${\displaystyle \sum_{k=0}^n a^k = \frac{a^{n+1}-1}{a-1}}$
- ${\displaystyle \sum_{k=0}^n 2^k = \frac{2^{n+1}-1}{2-1}=2^{n+1}-1}$

# References